[
{
	"uri": "//localhost:1313/",
	"title": "Kubernetes on AWS",
	"tags": [],
	"description": "",
	"content": "Kubernetes on AWS Tổng quan Chào các bạn, trong buổi Workshop với chủ đề \u0026ldquo;Kubernetes on AWS\u0026rdquo;, các bạn sẽ làm quen với việc triển khai một cụm K8s đơn giản và làm quen với việc Upgrade Cluster, Backup và Restore etcd và phân quyền cho user truy cập vào tài nguyên Kubernetes.\nNội dung Giới thiệu Các bước chuẩn bị Hands on Labs Dọn dẹp tài nguyên "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createvpc/",
	"title": "Chuẩn bị VPC và Security Group",
	"tags": [],
	"description": "",
	"content": "Trong bước này chúng ta sẽ tạo VPC và Security Group để triển khai cụm K8s.\nSau khi đăng nhập vào AWS Console các bạn bấm vào link sau:\nVPC Cloudformation Stack Các bạn bấm next để tiến hành deploy, quá trình sẽ mất vài giây. Sau khi deploy xong kết quả sẽ như hình bên dưới:\nNhư vậy bạn đã triển khai VPC K8s thành công.\nTạo Stack Cloudformation "
},
{
	"uri": "//localhost:1313/3-handsonlabs/3.1-createk8s/",
	"title": "Creating a Single-Master Cluster with Kubeadm",
	"tags": [],
	"description": "",
	"content": "Chào mừng các bạn đến bài lab triển khai K8s trên EC2 AWS dùng kubeadm.\nTrong bài lab này mình sẽ hướng dẫn các bạn triển khai cụm K8s.\nMục tiêu:\nHiểu được cách triển khai K8s dùng kubeadm trên EC2 AWS Xử lý lỗi nếu có Thiết lập môi trường\nMột số thông số yêu cầu trước khi cài đặt\nItem Version Link UBUNTU SERVER 22.04.3 https://ubuntu.com/download/server KUBERNETES 1.29.1 https://kubernetes.io/releases/ CONTAINERD 1.7.13 https://containerd.io/releases/ RUNC 1.1.12 https://github.com/opencontainers/runc/releases CNI PLUGINS 1.4.0 https://github.com/containernetworking/plugins/releases CALICO CNI 3.27.2 https://docs.tigera.io/calico/3.27/getting-started/kubernetes/quickstart Trong bài Lab này mình sử dụng 4 EC2:\nEC2 Qty AMI vCPU Memory ec2-cluster 1 Ubuntu 22.04 1 1GB ec2-control-plane 1 Ubuntu 22.04 2 4GB ec2-worker1 2 Ubuntu 22.04 1 1GB Các bạn truy cập vào link bên dưới, để dùng Cloudformation thiết lập môi trường nhé:\nVPC Cloudformation Stack Link sẽ tự động chuyển hướng về Cloudformation Console, các bạn thay đổi một số thông tin trước khi deploy:\nParameters Value Stack name [yourname]-lab1 EC2KeyPairName Select Keypair EC2 MemberHOL your_name Lưu ý: Các bạn phải chọn đúng tên mình không được chọn thay tên người khác!!!.\nCác bạn click Next\nSau khi đã kiểm tra kỹ càng các bạn click \u0026ldquo;Submit\u0026rdquo; để hệ thống tiến hành triển khai\nCác bạn đợi một vài phút để triển khai các tài nguyên cần thiết cho việc triển khai.\nSau khi Cloudformation đã chạy xong, chúng ta vào EC2 Console, lúc này sẽ có 4 EC2 đang Running\nChúng ta tiến hành remote vào từng EC2 để cấu hình:\nCó 2 cách để remote:\nCác bạn có thể remote trực tiếp trên trình duyệt. Các bạn có thể sử dụng một số công cụ terminal để login với keypair đã khai báo trước đó. Ở đây mình sẽ sử dụng remote bằng trình duyệt web.\nCấu hình Control-plane node Các bạn login vào EC2 Control-plane Node\nChuyển sang mode root\nsudo su Trong quá trình triển khai Cloudformation đã giúp chúng ta cài đặt một số thư viện có sẵn bạn có thể tham khảo script bên dưới\n#!/bin/bash apt update apt install unzip -y curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install # Load required kernel modules cat \u0026lt;\u0026lt;EOF | tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF modprobe overlay modprobe br_netfilter # Setup required sysctl params cat \u0026lt;\u0026lt;EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sysctl --system # Install containerd wget https://github.com/containerd/containerd/releases/download/v1.7.13/containerd-1.7.13-linux-amd64.tar.gz -P /tmp/ tar -C /usr/local -xzf /tmp/containerd-1.7.13-linux-amd64.tar.gz wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -P /etc/systemd/system/ systemctl daemon-reload systemctl enable --now containerd # Install runc wget https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64 -P /tmp/ install -m 755 /tmp/runc.amd64 /usr/local/sbin/runc # Install CNI plugins wget https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz -P /tmp/ mkdir -p /opt/cni/bin tar -C /opt/cni/bin -xzf /tmp/cni-plugins-linux-amd64-v1.4.0.tgz # Configure containerd mkdir -p /etc/containerd containerd config default | tee /etc/containerd/config.toml sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml systemctl restart containerd # Disable swap swapoff -a sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab # Install dependencies apt-get update apt-get install -y apt-transport-https ca-certificates curl gpg # Add Kubernetes apt repository mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /\u0026#39; | tee /etc/apt/sources.list.d/kubernetes.list # Update apt and install Kubernetes components apt-get update apt-get install -y kubelet=1.29.1-1.1 kubeadm=1.29.1-1.1 kubectl=1.29.1-1.1 apt-mark hold kubelet kubeadm kubectl Bây giờ chúng ta sẽ tiến hành khởi chạy Kubernetes bằng lệnh như sau:\n# Initialize the Kubernetes control plane kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.29.1 Sau khi init không lỗi nó sẽ thông báo successfully\nOutput\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.0.1.168:6443 --token bip3vm.s1h46hf1lczg623k \\ --discovery-token-ca-cert-hash sha256:432ae9d8d1f9174c0ffea6c3f015036b1b27e5f32506c4825daa147657a8ca5a Hiện tại Kubernetes đang run với root user, nếu chúng ta muốn thêm user để có thể sử dụng truy vấn đến Kubernetes thì ta thực hiện một số câu lệnh bên dưới:\n# Add kube config to ubuntu user. mkdir -p /home/ubuntu/.kube; cp -i /etc/kubernetes/admin.conf /home/ubuntu/.kube/config; chmod 755 /home/ubuntu/.kube/config # Set up kubeconfig for the root user export KUBECONFIG=/etc/kubernetes/admin.conf Lưu ý hiện tại user chạy với full quyền admin K8s, để giới hạn một số role ta sẽ thực hiện ở bài Lab 3.4\nTiếp theo chúng ta sẽ cài đặt Calico CNI cho K8s\n# Install Calico CNI kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/tigera-operator.yaml wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/custom-resources.yaml kubectl apply -f custom-resources.yaml Kiểm tra kết quả sau khi cài đặt\nTa dùng lệnh:\nkubectl get node Ta thấy hiện tại chỉ có một control-plane node đã join vào cụm K8s, lúc này ta sẽ join các Worker node vào cụm K8s sau bằng lênh:\n# Generate kubeadm join command and save to a file kubeadm token create --print-join-command Kết quả hiển thị 1 dòng lệnh, lúc này ta login vào các Worker Node để tiến hành cấu hình Join vào cụm K8s trên\nOutput\nkubeadm join 10.0.1.168:6443 --token s34g2r.rgpyp5ksuoyh29r9 --discovery-token-ca-cert-hash sha256:432ae9d8d1f9174c0ffea6c3f015036b1b27e5f32506c4825daa147657a8ca5a Cấu hình các Worker node Các bạn login vào EC2 Workers Node 1\nChuyển sang mode root\nsudo su Trong quá trình triển khai Cloudformation đã giúp chúng ta cài đặt một số thư viện có sẵn bạn có thể tham khảo script bên dưới:\n#!/bin/bash apt update apt install unzip -y curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install # Load required kernel modules cat \u0026lt;\u0026lt;EOF | tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF modprobe overlay modprobe br_netfilter # Setup required sysctl params cat \u0026lt;\u0026lt;EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sysctl --system # Install containerd wget https://github.com/containerd/containerd/releases/download/v1.7.13/containerd-1.7.13-linux-amd64.tar.gz -P /tmp/ tar -C /usr/local -xzf /tmp/containerd-1.7.13-linux-amd64.tar.gz wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -P /etc/systemd/system/ systemctl daemon-reload systemctl enable --now containerd # Install runc wget https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64 -P /tmp/ install -m 755 /tmp/runc.amd64 /usr/local/sbin/runc # Install CNI plugins wget https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz -P /tmp/ mkdir -p /opt/cni/bin tar -C /opt/cni/bin -xzf /tmp/cni-plugins-linux-amd64-v1.4.0.tgz # Configure containerd mkdir -p /etc/containerd containerd config default | tee /etc/containerd/config.toml sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/\u0026#39; /etc/containerd/config.toml systemctl restart containerd # Disable swap swapoff -a sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab # Install dependencies apt-get update apt-get install -y apt-transport-https ca-certificates curl gpg # Add Kubernetes apt repository mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /\u0026#39; | tee /etc/apt/sources.list.d/kubernetes.list # Update apt and install Kubernetes components apt-get update apt-get install -y kubelet=1.29.1-1.1 kubeadm=1.29.1-1.1 kubectl=1.29.1-1.1 apt-mark hold kubelet kubeadm kubectl Chúng ta tiến hành join cụm Worker node vào K8s ở trên bạn lênh:\nkubeadm join 10.0.1.168:6443 --token s34g2r.rgpyp5ksuoyh29r9 --discovery-token-ca-cert-hash sha256:432ae9d8d1f9174c0ffea6c3f015036b1b27e5f32506c4825daa147657a8ca5a Lệnh này được sinh ra từ Control-plane mà ta đã chạy trước đó\nChúng login lại control plane node và tiến hành kiểm tra xem worker node đã join vào cụm K8s chưa\nkubectl get node Ta thấy Worker 1 đã join vào cụm K8s\nCác bạn làm tương tự cho Worker 2 Node nhé\nKết quả đạt được\nNhư vậy bạn qua bài Lab trên đã giúp bạn cài đặt K8s sử dụng kubeadm.\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Kubernetes là gì Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\u0026ldquo;Kubernetes là một hệ thống mã nguồn mở dành cho việc tự động triển khai, thu phóng cũng như quản lý các ứng dụng đã được contanier\u0026rdquo;\nTừ Kubernetes bắt nguồn từ κυβερνήτης trong tiếng Hi Lạp, có nghĩ là người lái tàu. Với sự liên tưởng trong tâm trí của mình, ta có thể nghĩ Kubernetes giống như người thuyền trưởng trên một con tàu chứa đầy containers.\nKubernetes đôi khi được viết tắt là k8s (đọc là Kate\u0026rsquo;s), bởi có 8 kí tự giữa k và s. Nó lấy cảm hứng từ hệ thống Borg của Google, một bộ điều phối container và workload cho quá trình vận hành toàn cầu trong hơn một thập kỷ công ty này.\nNó là một dự án mã nguồn mở được viết bằng ngôn ngữ Go và được phân phối dưới giấy phép Apache License, phiên bản 2.0.\nKiến trúc của Kubernetes Một nút master hoặc nhiều hơn, đây là một bộ phận của control plane Một nút worker hoặc nhiền hơn Master (Control-plane) Node Nút master cung cấp môi trường cho control plane chịu trách nhiệm quản lý trạng thái của các cụm Kubernetes và nó là đầu nào đằng sau mọi quá trình vận hành trong cụm.\nThành phần control plane với vai trò vô cùng đặc thù trong bộ quản lý cụm. Để có thể giao tiếp với cụm Kubernetes, người sử dụng gửi các requests đến control plane thông qua các công cụ Command Line Interface (CLI), một Dashboard dưới dạng giao diện người dùng web (Web UI) , hoặc các API (Application Programming Interface).\nViệc giữ control plane chạy bằng mọi giá là tối quan trọng. Việc control plane ngừng hoạt động có thể dẫn đến downtime, thứ mà gây ra việc các dịch vụ gián đoạn và không thể cung cấp cho người dùng và có thể làm kinh doanh thua lỗ.\nĐể đảm bảo cho khả năng chịu lỗi của control plane, các bản sao của nút master được thêm vào cụm, được cấu hình ở chế độ HA (High-Availability, độ khả dụng cao). Trong khi chỉ một trong nút master được chuyên biệt để quản lý chủ động cụm, các thành phần control plane giữ trạng thái đồng bộ giữa các bản sao của nút master. Chính loại cấu hình này đã cung cấp khả năng tự phục hồi cho control plane của cụm, thứ có thể cho phép nút master đang ở trạng thái active xảy ra lỗi ở mức độ nào đó.\nĐể duy trình trạng thái của cụm, tất cả các dữ liệu về cấu hình của cụm đều được lưu trong etcd. etcd là một bộ lưu trữ dạng key-value phân tán, thứ chỉ giữ các dữ liệu liên quan đến trạng thái của cụm chứ không bao gồm dữ liệu workload của client. etcd có thể được cấu hình trên nút master (stacked topology) hoặc trên một host được chuyên biệt (external topology) để có thể giảm thiểu khả nằng mất mát dữ liệu được lưu trữ bằng cách tách riêng chúng khỏi các tác tử control plane khác.\nVới ở dạng stacked topology, các bản sao HA của nút master đảm bảo khả năng tự phục hồi của dữ liệu lưu trữ trong etcd. Tuy nhiên, đây không giống như trường hợp dùng etcd ở dạng external topology, khi mà host của etcd bắt buộc phải được tạo các nhân bản và tách rời để phục vụ cho chế đồ HA, cấu hình mà dẫn đến nhu cầu về các phần cứng bổ sung.\nMột nút master có thể có những thành phần control plane như sau:\n1. API Server (máy chủ API) Tất cả các tác vụ quản trị đều được phối hợp triển khai bởi kube-apiserver, thành phần control plane trung tâm được chạy trên nút master. API Server tiếp nhận các yếu cầu RESTful từ người dùng, tác tử hoạc các tác tử từ phía bên ngoài, sau đó xác nhận và xử lý chúng.\nTrong khi xử lý, API Server đọc trạng thái hiện tại của cụm Kubernetes từ bộ lưu trữ dữ liệu etcd và sau khi thực thi xong yêu cầu, trạng thái kết quả của cụm Kubernetes được lưu vào bộ lưu trữ dữ liệu dạng key-value phân tán để đảm bảo tính bền bỉ.\nAPI Server là thành phần master plane duy nhất có thể giao tiếp với bộ lưu trữ dữ liệu etcd. bao gồm cả đọc và ghi các thông tin trạng thái của cụm Kubernetes - hoạt động như một giao diện trung gian cho bất kì một tác tử control plane khác truy cập đến trạng thái của cụm.\nAPI Server có thể cấu hình và tùy biến một cách dễ dàng. Nó có thể mở rộng quy mô theo chiều ngang, nhưng nó cũng hỗ trợ việc bổ sung API Server phụ tùy chỉnh, một cấu hình biến API Server chính thành proxy cho tất cả API Server tùy chỉnh, thứ cấp và định tuyến tất cả các lệnh gọi RESTful đến chúng dựa trên các quy tắc được xác định tùy chỉnh.\n2. Scheduler (Bộ lập lịch) Vai trò của kube-scheduler là giao các các đối tượng workload mới, chẳng hạn như các pod cho các nút. Trong suốt quá trình lập lịch, các quyết định sẽ được tạo dựa trên trạng thái hiện tại của cụm Kubernetes và yêu cầu của các đối tượng mới.\nBộ lập lịch thu thập từ bộ lưu trữ dữ liệu, thông qua API server, dữ liệu sử dụng tài nguyên của từng nút worker trong cụm. Bộ lập lịch cũng có thể nhận từ API Server các requirements của các đối tượng mới, thứ là một phần của dữ liệu cấu hình của chúng.\nBộ lập lịch có khả năng cấu hình và tùy chỉnh cực kì cao dựa trên các scheduling policies, plugins, and profiles.\nMột bộ lập lịch thường cực kì quan trọng và phức tạp trong một cụm Kubernetes có nhiều nút. Tuy nhiên trong các cụm chỉ có duy nhất một nút, chẳng hạn như cụm được sử dụng làm ví dụ trong khóa học này, công việc của bộ lập lịch về cơ bản khá đơn giản.\n3. Controller Managers (Trình quản lý các controller) controller managers là các thành phần của control plane trên nút master, thứ mà chạy các controller để điều tiết trạng thái của cụm Kubernetes.\nkube-controller-manager chạy các controllers chịu trách nhiệm phản ứng khi các nút không khả dụng, để đảm bảo số pod như mong đợi, để tạo endpoints, service accounts, và API access tokens.\ncloud-controller-manager chạy các controllers chịu trách nhiệm tương tác với các hệ sinh thái cơ bản của bên cung cấp dịch vụ đám mây khi có nút trở nên không khả dụng cùng với đó là quản lý các container dữ liệu được cung cấp bởi các dịch vụ đám mấy và quản lý quá trình cân bằng tải và điều hướng.\n4. etcd Data Store (Kho dữ liệu) etcd là kho dữ liệu dạng key/value phân tán và có có tính nhất quán cao được sử dụng để đảm bảo tính bền bỉ của trạng thái của cụm Kubernetes. Dữ liệu mới được ghi vào kho lưu trữ chỉ bằng cách thêm vào cuối nó bởi vậy nên dữ liệu sẽ không bao giờ bị thay đổi trong đây. Dữ liệu lỗi thời được nén định kỳ để giảm thiểu dung lượng của kho dữ liệu.\nCông cụ quản lý dưới dạng CLI cuar etcd - etcdctl cung cấp khả năng backup, snapshot, and restore, những thứ đặc biệt tiện dụng đối với một cụm Kubernetes có một etcd duy nhất - thường thấy trong trong môi trường Phát triển.\nMột số công cụ khởi động cụm Kubernetes, chẳng hạn như kubeadm, mặc định sẽ cung cấp nút master với etcd dạng stacked, bơi ,à kho dữ liệu chạy song song và chia sẻ các tài nguyên với các thành phần control plane khác trên nút master đó.\nĐể cách ly kho dữ liệu khỏi các thành phần control plane, quy trình khởi động có thể được cấu hình cho cấu trúc liên kết etcd bên ngoài (external), nơi lưu trữ dữ liệu được cung cấp trên một máy chủ riêng biệt chuyên dụng, do đó giảm nguy cơ xảy ra lỗi etcd.\nCả cách cấu hình etcd dạng stacked hoặc external đều hỗ trợ cấu hình HA. etcd dựa trên thuật toán Raft Consensus điều này cho phép một tập hợp các máy hoạt động như một Group thống nhất có thể tồn tại sau những sự cố của một số thành viên của nó.\nTại bất kỳ thời điểm nào, một trong các nút trong Group sẽ là nút chính (master) và phần còn lại sẽ là nút theo dõi (follower). etcd xử lý khéo léo các cuộc bầu cử chính và có thể chịu được lỗi của nút, bao gồm cả lỗi của nút chính. Bất kỳ nút nào cũng có thể được coi là nút chính.\netcd được viết bằng ngôn ngữ lập trình Go. Trong Kubernetes, bên cạnh việc lưu trữ trang thái cụm, etcd cũng được sử dụng để lưu trữ đặc tả các dữ liệu cấu hình chẳng hạn như subnets, ConfigMaps, Secrets,\u0026hellip;\nNgoài ra, nút master có thể có:\nContainer Runtime Node Agent Proxy.\nWorker Node worker node cung cấp môi trường chạy cho các ứng dụng client.\nTrên một cụm Kubernetes có nhiều worker, lưu lượng mạng giữa các client users và các ứng dụng được triển khai trên các Pods được xử lý trực tiếp bởi các nút worker và nó không được điều hướng thông qua nút master.\nMột nút worker có các thành phần như sau:\n1. Container Runtime (môi trường chạy của container) Mặc dù Kubernetes được thiết kế như một engine có khả năng điều phối container, nó lại không có khả năng để có thể xử lý trực tiếp các container.\nĐể quản lý vòng đời của một container, Kubernetes cần có container runtime trên nuts mà một Pod và các container của nó được lập lịch. Kubernetes hỗ trợ các môi trường runtime sau:\nDocker - container runtime phổ biến nhất được sử dụng với Kubernetes (nhưng k8s đã ngừng hỗ trợ tại thời điểm gần đây) CRI-O - container runtime cho Kubernetes, nó cũng hỗ trợ Docker image registries containerd - container runtime đơn giản và di động cung cấp sự tự động đáng kể frakti - container runtime dựa trên hypervisor cho Kubernetes\n2. kubelet kubelet là tác tử chạy trên mỗi nút và giao tiếp với các thành phần của control plane từ nút master. Nó nhận thông tin định nghĩa Pod, chủ yếu từ API Server và tương tác với container runtime ở trên nút để chạy các container liên quan đến Pod đó.\nNó cũng theo dõi tình trạng và tài nguyên của các Pod đang chạy các containers.\n3. Proxy - kube-proxy kube-proxy là tác nhân mạng chạy trên mỗi nút chịu trách nhiệm cập nhật và bảo trì động của các luật điều phối mạng trên nút đó. Nó trừu tượng hóa chi tiết của quá trình hoạt động mạng của Pod và chuyển tiếp các kết nối đến Pods.\nNguồn: Tổng quan về kiến trúc của Kubernetes\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "\rBạn cần triển khai VPC và Security Group trước khi chạy Cloudformation.\nTrước khi đi vào các phần thực hành chúng ta cần phải setup môi trường, có như vậy chúng ta sẽ tập trung hơn về mục tiêu của các bài thực hành trên, giảm thiểu được thời gian cho việc triển khai các cụm K8s\nNội dung Chuẩn bị VPC và Security Group Tạo Stack Cloudformation "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createcfn/",
	"title": "Tạo Stack Cloudformation",
	"tags": [],
	"description": "",
	"content": "Tạo Stack Cloudformation Trong bước này chúng ta sẽ tạo môi trường để triển khai K8s trên AWS.\n"
},
{
	"uri": "//localhost:1313/3-handsonlabs/3.2-upgradecluster/",
	"title": "Upgrading a Single-Master Cluster Version with Kubeadm",
	"tags": [],
	"description": "",
	"content": "Tại sao phải cần cập nhật cho cluster cụm K8s Dưới đây sẽ là vài lý do lý giải vì sao chúng ta cần phải cập nhật cụm Kubernetes:\nĐảm bảo tính ổn định của cụm Cluster Trải nghiệm và sử dụng các tính năng mới Ngăn ngừa các vấn đề bảo mật, lỗ hổng phiên bản cũ etc. Do đó việc upgrade K8s là điều cần thiết, trong bài Lab này sẽ hướng dẫn các bạn nâng cấp cluster K8s.\nMục tiêu:\nHiểu được cách upgrade K8s cluster dùng kubeadm trên EC2 AWS Xử lý lỗi nếu có Thiết lập môi trường\nMột số thông số yêu cầu trước khi cài đặt\nItem Version Link UBUNTU SERVER 22.04.3 https://ubuntu.com/download/server KUBERNETES 1.29.1 https://kubernetes.io/releases/ CONTAINERD 1.7.13 https://containerd.io/releases/ RUNC 1.1.12 https://github.com/opencontainers/runc/releases CNI PLUGINS 1.4.0 https://github.com/containernetworking/plugins/releases CALICO CNI 3.27.2 https://docs.tigera.io/calico/3.27/getting-started/kubernetes/quickstart Trong bài Lab này mình sử dụng 4 EC2:\nEC2 Qty AMI vCPU Memory ec2-cluster 1 Ubuntu 22.04 1 1GB ec2-control-plane 1 Ubuntu 22.04 2 4GB ec2-worker1 2 Ubuntu 22.04 1 1GB Các bạn truy cập vào link bên dưới, để dùng Cloudformation thiết lập môi trường nhé:\nVPC Cloudformation Stack Link sẽ tự động chuyển hướng về Cloudformation Console, các bạn thay đổi một số thông tin trước khi deploy:\nParameters Value Stack name [yourname]-lab2 EC2KeyPairName Select Keypair EC2 MemberHOL your_name Lưu ý: Các bạn phải chọn đúng tên mình không được chọn thay tên người khác!!!.\nCác bạn click Next\nSau khi đã kiểm tra kỹ càng các bạn click \u0026ldquo;Submit\u0026rdquo; để hệ thống tiến hành triển khai\nCác bạn đợi một vài phút để triển khai các tài nguyên cần thiết cho việc triển khai.\nSau khi Cloudformation đã chạy xong, chúng ta vào EC2 Console, lúc này sẽ có 4 EC2 đang Running\nCác bạn login vào EC2 Cluster với user là ubuntu, kiểm tra K8s bằng lệnh sau:\nkubectl get node Như vậy ta đã triển khai thành công\nPhiên bản hiện tại của cụm K8s mà chúng ta cài đặt là v1.29.1 do đó bây giờ ta sẽ tiến hành nâng cấp:\nNâng cấp các control-plane node trước Sau đó nâng cấp các Worker node Cấu hình Control-plane node Các bạn login vào EC2 Control-plane Node\nBây giờ chúng ta sẽ bắt đầu upgrade k8s ở control plane trước\nTìm kiếm phiên bản hỗ trợ kubeadm sudo apt update sudo apt-cache madison kubeadm Kiểm tra phiên bản hiện tại kubeadm version Chọn phiên bản cần nâng cấp ở đây mình chọn phiên bản cần nâng câp là v1.29.8-1.1 sudo apt-mark unhold kubeadm sudo apt-get update sudo apt-get install -y kubeadm=1.29.8-1.1 sudo apt-mark hold kubeadm Sau khi đã upgrade kiểm tra phiên bản mới đã được cập nhật hay chưa kubeadm version Sau đó ta tiến hành verify upgrade plan sudo kubeadm upgrade plan Nếu tiến trình không phát sinh lỗi ta tiến hành upgrade kubeadm sudo kubeadm upgrade apply v1.29.8 Sau khi upgrade kubeadm thành công ta tiến hành upgrade kubelet và kubectl sudo apt-mark unhold kubelet kubectl sudo apt-get update sudo apt-get install -y kubelet=1.29.8-1.1 kubectl=1.29.8-1.1 sudo apt-mark hold kubelet kubectl Restart kubelet sudo systemctl daemon-reload sudo systemctl restart kubelet Kiểm tra cụm K8s kubectl get node Cấu hình Các Worker node Chúng ta cấu hình tuần tự cho từng worker node\nCác bạn login vào Worker Node 1 và tiến hành upgrade\nDrain Worker Node # Control-plane or cluster set: kubectl drain ip-10-0-1-232 --ignore-daemonsets kubectl get node Upgrade Worker Node sudo kubeadm upgrade node Upgrade kubelet và kubectl sudo apt-mark unhold kubelet kubectl sudo apt-get update sudo apt-get install -y kubelet=1.29.8-1.1 kubectl=1.29.8-1.1 sudo apt-mark hold kubelet kubectl Restart kubelet sudo systemctl daemon-reload sudo systemctl restart kubelet uncordon cho Worker Node # Control-plane or cluster set: kubectl uncordon ip-10-0-1-232 Các bạn làm tương tự cho Worker 2 Node nhé\nKết quả đạt được\nNhư vậy trong bài Lab này đã giúp các bạn upgrade cụm K8s thành công.\n"
},
{
	"uri": "//localhost:1313/3-handsonlabs/3.3-backuprestoreetcd/",
	"title": "Backing Up and Restoring etcd",
	"tags": [],
	"description": "",
	"content": "Tại sao phải cần Backup \u0026amp; Restore cho ETCD Trong kiến trúc Kubernetes, etcd là một phần không thể thiếu của cụm. Tất cả các đối tượng cụm và trạng thái của chúng được lưu trữ trong etcd. Một số điều bạn nên biết về etcd từ góc độ Kubernetes.\nNó là một kho lưu trữ key-value nhất quán, phân tán và an toàn. Hỗ trợ kiến trúc có tính khả dụng cao với stack etcd. Nó lưu trữ cấu hình cụm kubernetes, tất cả các đối tượng API, trạng thái đối tượng và service discovery. etc. Do đó việc sao lưu và backup etcd theo định kỳ là điều tất yếu, trong bài Lab này sẽ hướng dẫn các bạn thực hiện việc sao lưu và restore etcd.\nMục tiêu:\nHiểu được phương pháp backup và restore etcd. Xử lý lỗi nếu có Thiết lập môi trường\nSử dụng lại môi trường trong Lab3.2 Lưu ý: Các bạn xem lại Lab 3.2 để thiết lập môi trường.\nCác bạn login vào EC2 Control-plane Node với user là ubuntu, kiểm tra K8s bằng lệnh sau:\nkubectl get node Để giúp các bạn hiểu cách backup và restore etcd. Mình sẽ hướng dẫn các bạn deploy một số pod trên cụm K8s. Các bạn thực hiện các lệnh sau:\nkubectl run nginx-before-backup --image=nginx Như vậy ta đã tạo xong 1 pod nginx trước khi backup.\nTiếp theo ta tiến hành backup etcd Các bạn truy cập vào file: cd /etc/kubernetes/manifests/ Ta nhận thấy điều kiện để để tiến hành sao lưu etcd thì một số thông tin gồm phải có: Parameter Value \u0026ndash;endpoints ? \u0026ndash;cert ? \u0026ndash;key ? \u0026ndash;cacert ? Nhưng giá trị này sẽ được tìm thấy trong file: etcd.yaml và file kube-apiserver.yaml\nTừ đó ta có những giá trị:\nParameter Value Location \u0026ndash;endpoints 127.0.0.1:2379 kube-apiserver.yaml \u0026ndash;cert /etc/kubernetes/pki/etcd/server.crt etcd.yaml \u0026ndash;key /etc/kubernetes/pki/etcd/server.key etcd.yaml \u0026ndash;cacert /etc/kubernetes/pki/etcd/ca.crt etcd.yaml Sau khi đã có các giá trị tương ứng ta thực hiện backup cho etcd bằng lệnh sau:\nETCDCTL_API=3 sudo etcdctl --endpoints 127.0.0.1:2379 \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ snapshot save /opt/cluster_backup.db Nếu trong quá trình run lệnh báo lỗi ta tiến hành cài đặt bổ sung etcd-client packages\nTa tiến hành run lại command trên\nNhư vậy ta đã backup etcd thành công\nTiếp theo ta tạo thêm 1 Pods nginx sau khi đã backup etcd.\nkubectl run nginx-after-backup --image=nginx Lúc này ta thấy đã tạo 2 con pod 1 con trước khi backup và 1 con sau khi backup.\nBây giờ ta sẽ tiến hành recovery etcd db. Ta cũng làm tương tự một số thông số yêu cầu:\nParameter Value \u0026ndash;data-dir ? \u0026ndash;endpoints ? \u0026ndash;cert ? \u0026ndash;key ? \u0026ndash;cacert ? Nhưng giá trị này sẽ được tìm thấy trong file: etcd.yaml và file kube-apiserver.yaml\nTừ đó ta có những giá trị:\nParameter Value Location \u0026ndash;data-dir /var/lib/etcd etcd.yaml \u0026ndash;endpoints 127.0.0.1:2379 kube-apiserver.yaml \u0026ndash;cert /etc/kubernetes/pki/etcd/server.crt etcd.yaml \u0026ndash;key /etc/kubernetes/pki/etcd/server.key etcd.yaml \u0026ndash;cacert /etc/kubernetes/pki/etcd/ca.crt etcd.yaml ETCDCTL_API=3 sudo etcdctl --data-dir /var/lib/etcd \\ --endpoints 127.0.0.1:2379 \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ snapshot restore /opt/cluster_backup.db \u0026gt; restore.txt 2\u0026gt;\u0026amp;1 Ta thấy báo lỗi trong quá trình restore\nError: data-dir \u0026ldquo;/var/lib/etcd\u0026rdquo; exists\nChúng ta tiến hành remove file\nsudo rm -rf /var/lib/etcd Sau đó tiến hành recovery lại\nTiến hành kiểm tra lại\nkubectl get pod Lúc này ta chỉ còn thấy 1 pod nginx before backup\nNhư vậy là đã thành công trong việc sao lưu và restore etcd.\n"
},
{
	"uri": "//localhost:1313/3-handsonlabs/",
	"title": "Hands on Labs ",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ thực hiện các bài thực hành về việc upgrade cluster, backup and restore etcd, và phân quyền truy cập cho K8s bằng RBAC.\nNội dung 3.1. Creating a Single-Master Cluster with Kubeadm 3.2. Upgrading a Single-Master Cluster Version with Kubeadm 3.3. Backing Up and Restoring etcd 3.4. Regulating Access to API Resources with RBAC\n"
},
{
	"uri": "//localhost:1313/3-handsonlabs/3.4-rbacuser/",
	"title": "Regulating Access to API Resources with RBAC",
	"tags": [],
	"description": "",
	"content": "Tại sao lại cần phải phân quyền quản trị trong K8s Hãy tưởng tượng ta gặp trường hợp như sau. Công ty ta có một Kubernetes Cluster với hai môi trường pro và dev. Với pro cho môi trường sản phẩm thực tế và dev dùng để phát triển sản phẩm.\nTrước giờ bạn chỉ làm một mình và có toàn bộ quyền để quản lý Kubernetes Cluster, mỗi lần Developer muốn làm gì đều phải thông qua bạn, kể cả xem logs.\nSếp bạn thấy vậy hơi bất tiện nên kêu bạn tìm cách làm thế nào để mấy bạn Developer có thể tự liệt kê ứng dụng và xem poda ở dưới môi trường dev.\nGiải pháp Ta dùng Role-based access control (RBAC) để giải quyết vấn đề trên.\nMục tiêu:\nHiểu được phương pháp phân quyền quản trị tài nguyên K8s thông qua user Xử lý lỗi nếu có Thiết lập môi trường\nSử dụng lại môi trường trong Lab3.2 Lưu ý: Các bạn xem lại Lab 3.2 để thiết lập môi trường.\nCác bạn login vào EC2 Cluster Node với user là ubuntu, kiểm tra K8s bằng lệnh sau:\nkubectl get node Tiếp theo ta tiến hành phân quyền cho K8s sử dụng RBAC Trên EC2 Cluster tạo 2 user, user \u0026ldquo;devuser\u0026rdquo; sudo su sudo adduser devuser #password: admin123 sudo usermod -aG sudo devuser Tạo certification cho user devuser: sudo su - ubuntu openssl genrsa -out devuser.pem Tạo Create a Certificate Signing Request (CSR) cho devuser:\nsudo su - ubuntu openssl req -new -key devuser.pem -out devuser.csr -subj \u0026#34;/CN=devuser\u0026#34; Encode base64 CSR user devuser: cat devuser.csr | base64 | tr -d \u0026#34;\\n\u0026#34; # output csr encode-base64 text Output\nLS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1Z6Q0NBVDhDQVFBd0VqRVFNQTRHQTFVRUF3d0haR1YyZFhObGNqQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFQZ3dYb3J0V3BCemsxSHg3eVYyM1NkSW1rT1p3Ui90QzNja0ZMOWRLcDhTCk1HU3dSQy9VeS9lMlY5OVlSS3A1Qk5IeXhsekNYQ3QwRCs1UHZqR2hwWnlURjlBYkFtQXduN01RRWtWT29sbXoKRC9aTEdtQ2lhOFh0N1RTYnUxbFMweUtGWkcwRDNYdEtZYVA1Rnk0SWJpWEsxd0luSnk1Vk9ycnBvYkk4UG96WgoxOWlmek9HcGFGRythUEg2d0ZPa0NveWU5cXJBaFg5dUpjVzVJSlFPRnNBQ2RUWjVCcFZBOE5jUkdDZzF3K01DClFWSjdXZGtVNjc5bEllTjlZSFZ6UVBOK1gxdkhnSElaTG4wSXcxcWVNUUFWMG5odlBnNTA3MXZEL1lhQkxFcGMKL0VNKzlVSUxFQ2RSMTB1MWtwMnV3WmxGQ3QraXg0UlJ0R1htbHlBWlEzMENBd0VBQWFBQU1BMEdDU3FHU0liMwpEUUVCQ3dVQUE0SUJBUUFHa3lDMUxKMk45QUNoRDk1L1VOdGxCQ1ZpbDE4MVRSc3pTOTFhbzJYM1BudlFVOUFrCmJNekJaY3lKSGo0cnY4ZkFEalV6WnJQb205cjJCZnIvbXRVdmduZzRBTk90U0ViUWY0UHN6SEdYdVRianJ1aU8KcFo5VWxBdE9iOUpWQm1ya1hIdjFFVXRDdyt5azZlMkJjUVZ1T01SdXdBWGFYSDBKR0tlcENsemlqM21CSnhzOQorYTFNaVdCSmpLWGMzc3VCaVovNWFMZXp6aG1uR3JBU0tTaEFBMjVOcGtoR0FTc3NvMEVWQUpmclBXRVNEK2NkCm81bVFxT1QvdlhvTDh0cFVyMUF1ZTNiaFA3RWI0TGdqaVp3U29mVjh6ZDZnbFBDVG1QQjB5blBnYXBGcXFmOWwKOWhnZEl4dXhRdFZWa2s5UlZ6ZTB6UUhwT0s4WGtwSjRIcEVSCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo= \u0026ldquo;Đăng ký \u0026ldquo;\u0026ldquo;CertificateSigningRequest\u0026rdquo;\u0026rdquo; trên cụm K8s Tạo file \u0026ldquo;\u0026ldquo;devuserSigningRequest.yaml\u0026rdquo;\u0026rdquo;:\napiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: devuser spec: request: \u0026lt;base64_encoded_csr\u0026gt; signerName: kubernetes.io/kube-apiserver-client expirationSeconds: 86400 # one day usages: - digital signature - key encipherment - client auth Trong đó \u0026lt;base64_encoded_csr\u0026gt; chính là output của bước 3.\nSau đó chạy lệnh đăng ký trên K8s:\nkubectl apply -f devuserSigningRequest.yaml Kiểm tra trạng thái csr:\nkubectl get csr Ta thấy trong cột điều kiện đang thể hiện trạng thái pending chờ được approve từ admin: Approve cho user devuser\nkubectl certificate approve devuser Sau khi appove tiến hành kiểm tra lại csr:\nkubectl get csr/devuser Tạo xác thực cho user devuser: kubectl get csr/devuser -o jsonpath=\u0026#34;\u0026#34;{.status.certificate}\u0026#34;\u0026#34; | base64 -d \u0026gt; devuser.crt Tạo config file cho user devuser: Lấy server url:\nkubectl config view kubectl config view -o jsonpath=\u0026#34;{.clusters[0].cluster.server}\u0026#34; #output is url kubernetes api Output\nhttps://10.0.1.45:6443 Cấu hình config cho user devuser\nsudo kubectl --kubeconfig ~/.kube/config-devuser config set-cluster devuser-cluster --insecure-skip-tls-verify=true --server=https://10.0.1.45:6443 Tạo credentials cho user devuser: sudo kubectl --kubeconfig ~/.kube/config-devuser config set-credentials devuser --client-certificate=devuser.crt --client-key=devuser.pem --embed-certs=true Tạo context cho user \u0026ldquo;devuser\u0026rdquo;: sudo kubectl --kubeconfig ~/.kube/config-devuser config set-context devuser-context --cluster=devuser-cluster --user=devuser Đăng ký context cho cụm K8s: sudo kubectl --kubeconfig ~/.kube/config-devuser config use-context devuser-context Thêm quyền xr cho file config-devuser: sudo chmod +xr config-devuser Tạo roles và rolebindings cho user devuser: Writing RBAC Rules\nTạo file role-devuser.yaml:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # indicates the core API group resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] Create role:\nkubectl apply -f role-devuser.yaml Tạo rolebinding cho role đã tạo trước đó: tạo file devuser-pod-reader-rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: devuser apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io Create role:\nkubectl apply -f devuser-pod-reader-rolebinding.yaml Dùng quyền admin trong cụm ubuntu tạo 1 pod trên namespace default: kubectl run nginx --image=nginx Sử dụng config-devuser kiểm tra pods trên namspace default và kube-system\nkubectl --kubeconfig ~/.kube/config-devuser get pod -n default kubectl --kubeconfig ~/.kube/config-devuser get pod -n kube-system Chuyển mode user devuser trong EC2 Cluster copy file config-devuser trong folder .kube Sau khi copy đổi tên file config-devuser sang config.\nsudo su - devuser mkdir .kube cd .kube/ sudo cp /home/ubuntu/.kube/config-devuser /home/devuser/.kube/ sudo mv config-devuser config Tiến hành kiểm tra với user devuser: kubectl get pods -n default kubectl get pods -n kube-system Như vậy thông qua bài Lab trên bạn đã phân quyền thành công.\n"
},
{
	"uri": "//localhost:1313/4-cleanup/",
	"title": "Dọn dẹp tài nguyên  ",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ tiến hành các bước sau để xóa các tài nguyên chúng ta đã tạo trong bài thực hành này.\nXóa S3 Bucket Truy cập giao diện quản trị dịch vụ S3 Click chọn S3 bucket chúng ta đã tạo cho bài thực hành. ( Ví dụ : k8s-lab5-ap-southeast-1-data308 ) Click Empty. Điền permanently delete, sau đó click Empty để tiến hành xóa object trong bucket. Click Exit. Sau khi xóa hết object trong bucket, click Delete\nĐiền tên S3 bucket, sau đó click Delete bucket để tiến hành xóa S3 bucket.\nXóa Cloudformation Stack Truy cập giao diện quản trị dịch vụ Cloudformation. Click Stack chúng ta đã tạo cho bài thực hành. Click Delete. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]